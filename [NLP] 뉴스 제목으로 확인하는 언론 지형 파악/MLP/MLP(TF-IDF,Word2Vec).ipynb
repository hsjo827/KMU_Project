{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import torch\n",
    "import gensim\n",
    "import random\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import torch.nn as nn\n",
    "from kiwipiepy import Kiwi\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "from gensim.models import Word2Vec\n",
    "from kiwipiepy.utils import Stopwords\n",
    "from selenium.webdriver.common.by import By\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\4221925947.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  progressive_df['Label'] = 1\n",
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\4221925947.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conservative_df['Label'] = 0\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 불러오기  \n",
    "filename = 'news_articles_후쿠시마.csv'\n",
    "mlp_keyword = pd.read_csv(filename)\n",
    "\n",
    "# 파일명에서 키워드 추출 (예: 후쿠시마 오염수,윤석열,이재명....)\n",
    "keyword = os.path.splitext(filename)[0].split('_')[-1]\n",
    "\n",
    "# 진보,보수 성향 및 기타 언론사 리스트\n",
    "progressive_media = ['한겨레', '경향신문', '오마이뉴스']\n",
    "conservative_media = ['조선일보','동아일보','중앙일보']\n",
    "test_media = ['연합뉴스', '매일경제', '머니투데이']\n",
    "\n",
    "# 진보 성향 데이터프레임\n",
    "progressive_df = mlp_keyword[mlp_keyword['Source'].isin(progressive_media)]\n",
    "\n",
    "# 보수 성향 데이터프레임\n",
    "conservative_df = mlp_keyword[mlp_keyword['Source'].isin(conservative_media)]\n",
    "\n",
    "# test 언론사 데이터프레임\n",
    "test_media_df = mlp_keyword[mlp_keyword['Source'].isin(test_media)]\n",
    "\n",
    "\n",
    "# Progressive = 1, Conservative = 0\n",
    "progressive_df['Label'] = 1\n",
    "conservative_df['Label'] = 0\n",
    "\n",
    "other_media = ['SBS Biz','한국경제TV','뉴스1','JTBC','채널A','MBN','SBS','MBC','YTN','뉴시스', \n",
    "                'KBS','TV조선','연합뉴스TV','매경이코노미','한경비즈니스','월간산','한겨레21',\n",
    "                '이코노미스트','더스쿠프','주간경향','신동아','시사저널','주간조선','시사IN',\n",
    "                '주간동아','농민신문','기자협회보','코메디닷컴','코리아헤럴드','코리아중앙데일리',\n",
    "                '일다','뉴스타파','동아사이언스','헬스조선','여성신문','서울신문','문화일보','세계일보',\n",
    "                '한국일보','국민일보','아이뉴스24','디지털데일리','데일리안','머니S','블로터','더팩트',\n",
    "                '프레시안','지디넷코리아','미디어오늘','디지털타임스','노컷뉴스','전자신문','서울경제',\n",
    "                '헤럴드경제','비즈워치','아시아경제','파이낸셜뉴스','조선비즈','이데일리','조세일보','한국경제']\n",
    "\n",
    "# 전체 언론사명 리스트\n",
    "all_media = progressive_media + conservative_media + test_media + other_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\716994766.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  progressive_df['Title'] = progressive_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)\n",
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\716994766.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conservative_df['Title'] = conservative_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)\n",
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\716994766.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_media_df['Title'] = test_media_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)\n"
     ]
    }
   ],
   "source": [
    "# 기사 제목에서 []로 감싸져 있는 텍스트 및 언론사 이름 제거 함수\n",
    "def remove_bracketed_text_and_media(title, media_list):\n",
    "    title = re.sub(r'\\[.*?\\]', '', title).strip()\n",
    "    for media in media_list:\n",
    "        title = title.replace(media, '').strip()\n",
    "    return title\n",
    "\n",
    "# 모든 데이터프레임에서 제목 수정\n",
    "progressive_df['Title'] = progressive_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)\n",
    "conservative_df['Title'] = conservative_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)\n",
    "test_media_df['Title'] = test_media_df['Title'].apply(remove_bracketed_text_and_media, media_list=all_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 BERT 모델을 사용하여 입력된 문장을 변형 (Data Augmentation)\n",
    "# random_masking_replacement : 무작위로 단어를 마스킹하고 복원하는 방식\n",
    "# random_masking_insertion : 문장 내에 무작위로 마스크 토큰을 삽입하고 이를 복원하는 방식\n",
    "\n",
    "\n",
    "class BERT_Augmentation:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'monologg/koelectra-base-v3-generator'\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.unmasker = pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer)\n",
    "        random.seed(42)\n",
    "\n",
    "    # 무작위로 단어를 마스킹하고 복원하는 방식\n",
    "    def random_masking_replacement(self, sentence, ratio=0.15):\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        \n",
    "        # 품질 유지를 위해, 문장의 어절 수가 4 이하라면 원문장을 그대로 리턴\n",
    "        if len(sentence.split()) <= 4:\n",
    "            return sentence\n",
    "\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "\n",
    "        unmask_sentence = sentence\n",
    "        # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음\n",
    "        # 따라서 처음 단어(0번째)와 끝 단어를 제외하고 무작위 인덱스를 선택함\n",
    "        random_idx = random.randint(1, len(unmask_sentence.split()) - span)\n",
    "        \n",
    "        unmask_sentence = unmask_sentence.split()\n",
    "        cache = []\n",
    "        for _ in range(span):\n",
    "            # 처음과 끝 부분을 [MASK]로 치환 후 추론할 때의 품질이 좋지 않음\n",
    "            # 따라서 처음 단어(0번째)와 끝 단어를 제외하고 무작위 인덱스를 선택함\n",
    "            while cache and random_idx in cache:\n",
    "                random_idx = random.randint(1, len(unmask_sentence) - 2)\n",
    "            cache.append(random_idx)\n",
    "            unmask_sentence[random_idx] = mask\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0]['sequence']\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "        unmask_sentence = \" \".join(unmask_sentence)\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()\n",
    "    \n",
    "    # 문장 내에 무작위로 마스크 토큰을 삽입하고 이를 복원하는 방식\n",
    "    def random_masking_insertion(self, sentence, ratio=0.15):\n",
    "        span = int(round(len(sentence.split()) * ratio))\n",
    "        mask = self.tokenizer.mask_token\n",
    "        unmasker = self.unmasker\n",
    "        \n",
    "        # Recover\n",
    "        unmask_sentence = sentence\n",
    "        \n",
    "        for _ in range(span):\n",
    "            unmask_sentence = unmask_sentence.split()\n",
    "            random_idx = random.randint(0, len(unmask_sentence)-1)\n",
    "            unmask_sentence.insert(random_idx, mask)\n",
    "            unmask_sentence = unmasker(\" \".join(unmask_sentence))[0]['sequence']\n",
    "\n",
    "        unmask_sentence = unmask_sentence.replace(\"  \", \" \")\n",
    "\n",
    "        return unmask_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "증강된 문장 예시:\n",
      "Original: 기시다 만나러 서울 가는 이용수 할머니 \"법적 배상 촉구할 것\"\n",
      "Augmented (Replacement): 기시다 만나러 서울 가는 이용수 할머니 \" 배상할 것 \"\n",
      "Augmented (Insertion): 기시다 만나러 서울 가는 이용수 할머니 \" 법적인 배상을 촉구할 것 \"\n",
      "\n",
      "Original: 일본 오염수 방류 규탄 결의문 읽다가... 강제 퇴장당한 시의원\n",
      "Augmented (Replacement): 일본 오염수 방류 규탄 결의문 읽다가.... 퇴장당한 시의원\n",
      "Augmented (Insertion): 일본 오염수 방류 규탄 결의문 읽다가.... 강제 퇴장당한 시의원\n",
      "\n",
      "Original: \"'방사성 오염수 투기' 일본을 국제해양법 재판소에 제소하라\"\n",
      "Augmented (Replacement): \"'방사성 오염수 투기'일본을 국제해양법 재판소에 \"\n",
      "Augmented (Insertion): \"'방사성 오염수 투기'일본을 국제 국제해양법 재판소에 제소하라 \"\n",
      "\n",
      "Original: 한·중·러만 '오염수'로 부른다?... 미국이 오히려 예외\n",
      "Augmented (Replacement): 한 · 중 · 러만'오염수'로 부른다?.... 오히려 예외\n",
      "Augmented (Insertion): 한 · 중 · 러만'오염수'로만 부른다?... 미국이 오히려 예외\n",
      "\n",
      "Original: 이종호 <> 기자, '올해의 주목할만한 데이터저널리스트상' 수상\n",
      "Augmented (Replacement): 이종호 < > 기자,'올해의 주목할만한 데이터저널리스트상'발표\n",
      "Augmented (Insertion): 이종호 < > 기자, 올해'올해의 주목할만한 데이터저널리스트상'수상\n",
      "\n",
      "진보 성향 데이터 수: 848\n",
      "보수 성향 데이터 수: 848\n",
      "통합 데이터프레임 데이터 수: 1696\n"
     ]
    }
   ],
   "source": [
    "# BERT 증강 수행\n",
    "augmenter = BERT_Augmentation()\n",
    "\n",
    "# 진보(progressive)와 보수(conservative) 데이터의 수를 비교\n",
    "# 데이터 수가 적은 쪽의 수를 증가시키기 위해 증강할 데이터 수 결정\n",
    "\n",
    "# 데이터 프레임에서 무작위로 문장을 선택하여, 두 가지 증강 방법 사용\n",
    "# random_masking_replacement: 문장에서 무작위로 단어를 마스킹하고 복원하는 방식\n",
    "# random_masking_insertion: 문장 내에 무작위로 마스크 토큰을 삽입하고 이를 복원하는 방식\n",
    "\n",
    "# 선택된 각 문장에 대해 두 가지 증강 방법을 각각 한 번씩 적용하여 새로운 두 개의 문장을 생성\n",
    "# 만약 증강해야 될 데이터 수가 홀수이면 첫 번째 증강 방법(random_masking_replacement)만 한 번 더 수행\n",
    "# 이렇게 생성된 문장들을 데이터프레임에 추가\n",
    "\n",
    "# 최종적으로 데이터 수가 같아짐\n",
    "def augment_dataframe(df, augmenter, augment_count):\n",
    "    new_rows = []\n",
    "    for _ in range(augment_count // 2):  # Each iteration adds two new rows\n",
    "        random_idx = random.randint(0, len(df) - 1)\n",
    "        original_text = df.iloc[random_idx]['Title']\n",
    "        \n",
    "        augmented_text = augmenter.random_masking_replacement(original_text)\n",
    "        new_row = df.iloc[random_idx].copy()\n",
    "        new_row['Title'] = augmented_text\n",
    "        new_rows.append(new_row)\n",
    "        \n",
    "        augmented_text = augmenter.random_masking_insertion(original_text)\n",
    "        new_row = df.iloc[random_idx].copy()\n",
    "        new_row['Title'] = augmented_text\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    if augment_count % 2 != 0:\n",
    "        random_idx = random.randint(0, len(df) - 1)\n",
    "        original_text = df.iloc[random_idx]['Title']\n",
    "        augmented_text = augmenter.random_masking_replacement(original_text)\n",
    "        new_row = df.iloc[random_idx].copy()\n",
    "        new_row['Title'] = augmented_text\n",
    "        new_rows.append(new_row)\n",
    "        \n",
    "    return pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "# 진보와 보수 데이터 크기 비교 및 증강할 데이터 수 결정\n",
    "progressive_count = len(progressive_df)\n",
    "conservative_count = len(conservative_df)\n",
    "\n",
    "if progressive_count < conservative_count:\n",
    "    augment_count = conservative_count - progressive_count\n",
    "    progressive_df = augment_dataframe(progressive_df, augmenter, augment_count)\n",
    "else:\n",
    "    augment_count = progressive_count - conservative_count\n",
    "    conservative_df = augment_dataframe(conservative_df, augmenter, augment_count)\n",
    "\n",
    "# 데이터프레임 결합\n",
    "combined_df = pd.concat([progressive_df, conservative_df], ignore_index=True)\n",
    "\n",
    "# 증강된 문장 출력\n",
    "print(\"증강된 문장 예시:\")\n",
    "for i in range(5):\n",
    "    original_text = progressive_df.iloc[i]['Title']\n",
    "    augmented_text_replacement = augmenter.random_masking_replacement(original_text)\n",
    "    augmented_text_insertion = augmenter.random_masking_insertion(original_text)\n",
    "    print(f\"Original: {original_text}\")\n",
    "    print(f\"Augmented (Replacement): {augmented_text_replacement}\")\n",
    "    print(f\"Augmented (Insertion): {augmented_text_insertion}\")\n",
    "    print()\n",
    "\n",
    "# 결과 확인\n",
    "print(\"진보 성향 데이터 수:\", len(progressive_df))\n",
    "print(\"보수 성향 데이터 수:\", len(conservative_df))\n",
    "print(\"통합 데이터프레임 데이터 수:\", len(combined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwi 초기화 및 불용어 설정\n",
    "kiwi = Kiwi(typos='basic')\n",
    "stopwords = Stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\\s]', '', text) #한글과 공백을 제외한 모든 문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text) #연속된 공백을 단일 공백으로 변환\n",
    "    return text.strip() #양쪽 끝의 공백 문자 제거\n",
    "\n",
    "\n",
    "# 토큰화 및 불용어 제거 함수\n",
    "def tokenize_and_remove_stopwords(text, kiwi, stopwords):\n",
    "    tokens = kiwi.tokenize(text, stopwords=stopwords)\n",
    "    return [token.form for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\1831875621.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Clean_Title'] = df['Title'].apply(clean_text)\n",
      "C:\\Users\\7johs\\AppData\\Local\\Temp\\ipykernel_22732\\1831875621.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Tokenized_Title'] = df['Clean_Title'].apply(lambda x: ' '.join(tokenize_and_remove_stopwords(x, kiwi, stopwords)))\n"
     ]
    }
   ],
   "source": [
    "# 전처리 및 토큰화\n",
    "def preprocess_dataframe(df):\n",
    "    df['Clean_Title'] = df['Title'].apply(clean_text)\n",
    "    df['Tokenized_Title'] = df['Clean_Title'].apply(lambda x: ' '.join(tokenize_and_remove_stopwords(x, kiwi, stopwords)))\n",
    "    return df\n",
    "\n",
    "combined_df = preprocess_dataframe(combined_df)\n",
    "test_media_df = preprocess_dataframe(test_media_df)\n",
    "\n",
    "# 'Clean_Title' 값이 없는 행 제거\n",
    "combined_df = combined_df[combined_df['Clean_Title'].str.strip() != '']\n",
    "test_media_df = test_media_df[test_media_df['Clean_Title'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source\n",
       "동아일보     524\n",
       "한겨레      336\n",
       "경향신문     290\n",
       "중앙일보     244\n",
       "오마이뉴스    222\n",
       "조선일보      80\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Source\n",
       "매일경제     179\n",
       "머니투데이    143\n",
       "연합뉴스       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전처리 된 언론사별 데이터 수 계산 함수\n",
    "def count_articles_by_media(df, media_list):\n",
    "    return df[df['Source'].isin(media_list)]['Source'].value_counts()\n",
    "\n",
    "# 각 미디어 그룹의 데이터프레임에서 언론사별 데이터 수 계산\n",
    "combined_df_counts = count_articles_by_media(combined_df, progressive_media+conservative_media)\n",
    "test_media_df_counts = count_articles_by_media(test_media_df, test_media)\n",
    "\n",
    "display(combined_df_counts,test_media_df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(combined_df['Tokenized_Title'])\n",
    "\n",
    "X = tfidf_matrix\n",
    "y = combined_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# Word2Vec 적용 시 코드\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "sentences = combined_df['Tokenized_Title'].tolist()\n",
    "word2vec_model = gensim.models.Word2Vec(sentences, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "# 문서 벡터화 함수\n",
    "def get_document_vector(tokens, model):\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "# 문서 벡터화\n",
    "combined_df['Vector'] = combined_df['Tokenized_Title'].apply(lambda x: get_document_vector(x, word2vec_model))\n",
    "test_media_df['Vector'] = test_media_df['Tokenized_Title'].apply(lambda x: get_document_vector(x, word2vec_model))\n",
    "\n",
    "X = np.vstack(combined_df['Vector'].values)\n",
    "y = combined_df['Label'].values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "#Word2Vec 적용 시 코드\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class ImprovedNet(nn.Module):\n",
    "    def __init__(self, in_dim, h1_dim, h2_dim, h3_dim, h4_dim, out_dim):\n",
    "        super(ImprovedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h1_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(h2_dim)\n",
    "        self.fc3 = nn.Linear(h2_dim, h3_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(h3_dim)\n",
    "        self.fc4 = nn.Linear(h3_dim, h4_dim)\n",
    "        self.bn4 = nn.BatchNorm1d(h4_dim)\n",
    "        self.fc5 = nn.Linear(h4_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.7315\n",
      "Epoch [2/30], Loss: 0.6741\n",
      "Epoch [3/30], Loss: 0.5730\n",
      "Epoch [4/30], Loss: 0.4171\n",
      "Epoch [5/30], Loss: 0.3184\n",
      "Epoch [6/30], Loss: 0.2177\n",
      "Epoch [7/30], Loss: 0.1592\n",
      "Epoch [8/30], Loss: 0.1272\n",
      "Epoch [9/30], Loss: 0.1008\n",
      "Epoch [10/30], Loss: 0.0750\n",
      "Epoch [11/30], Loss: 0.0849\n",
      "Epoch [12/30], Loss: 0.0550\n",
      "Epoch [13/30], Loss: 0.0531\n",
      "Epoch [14/30], Loss: 0.0358\n",
      "Epoch [15/30], Loss: 0.0405\n",
      "Epoch [16/30], Loss: 0.0346\n",
      "Epoch [17/30], Loss: 0.0381\n",
      "Epoch [18/30], Loss: 0.0396\n",
      "Epoch [19/30], Loss: 0.0331\n",
      "Epoch [20/30], Loss: 0.0346\n",
      "Epoch [21/30], Loss: 0.0388\n",
      "Epoch [22/30], Loss: 0.0333\n",
      "Epoch [23/30], Loss: 0.0281\n",
      "Epoch [24/30], Loss: 0.0397\n",
      "Epoch [25/30], Loss: 0.0198\n",
      "Epoch [26/30], Loss: 0.0175\n",
      "Epoch [27/30], Loss: 0.0363\n",
      "Epoch [28/30], Loss: 0.0296\n",
      "Epoch [29/30], Loss: 0.0310\n",
      "Epoch [30/30], Loss: 0.0386\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedNet(in_dim=X_train.shape[1], h1_dim=512, h2_dim=256, h3_dim=128, h4_dim=64, out_dim=2).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.59%\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_media_df로 softmax 값 도출\n",
    "test_media_tfidf_matrix = vectorizer.transform(test_media_df['Tokenized_Title'])\n",
    "\n",
    "test_media_X_tensor = torch.tensor(test_media_tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "test_media_dataset = TensorDataset(test_media_X_tensor)\n",
    "test_media_loader = DataLoader(test_media_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Word2Vec 적용 시 코드\n",
    "\n",
    "# test_media_df 벡터화\n",
    "test_media_X = np.vstack(test_media_df['Vector'].values)\n",
    "\n",
    "# 텐서로 변환\n",
    "test_media_X_tensor = torch.tensor(test_media_X, dtype=torch.float32)\n",
    "test_media_dataset = TensorDataset(test_media_X_tensor)\n",
    "test_media_loader = DataLoader(test_media_dataset, batch_size=32, shuffle=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Progressive_Probability</th>\n",
       "      <th>Conservative_Probability</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>매일경제</td>\n",
       "      <td>0.755188</td>\n",
       "      <td>0.244812</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>머니투데이</td>\n",
       "      <td>0.733527</td>\n",
       "      <td>0.266473</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>0.823134</td>\n",
       "      <td>0.176866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source  Progressive_Probability  Conservative_Probability  Count\n",
       "0   매일경제                 0.755188                  0.244812    179\n",
       "1  머니투데이                 0.733527                  0.266473    143\n",
       "2   연합뉴스                 0.823134                  0.176866      1"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 수행 및 softmax 값 도출\n",
    "model.eval()\n",
    "softmax = nn.Softmax(dim=1)\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_media_loader:\n",
    "        inputs = inputs[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = softmax(outputs)\n",
    "        predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "# 결과 데이터프레임에 추가\n",
    "test_media_df['Progressive_Probability'] = [p[1] for p in predictions]\n",
    "test_media_df['Conservative_Probability'] = [p[0] for p in predictions]\n",
    "\n",
    "\n",
    "# test meida 언론사별 평균 softmax 값과 데이터 수 계산\n",
    "result = test_media_df.groupby('Source').agg(\n",
    "    Progressive_Probability=('Progressive_Probability', 'mean'),\n",
    "    Conservative_Probability=('Conservative_Probability', 'mean'),\n",
    "    Count=('Source', 'size')\n",
    ").reset_index()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to test_media_predictions(TF-IDF)_후쿠시마 오염수_accuracy_85.59.csv\n"
     ]
    }
   ],
   "source": [
    "# 결과 파일 저장\n",
    "result_filename = f\"test_media_predictions(TF-IDF)_{keyword}_accuracy_{test_accuracy:.2f}.csv\"\n",
    "result.to_csv(result_filename, index=False)\n",
    "print(f\"Result saved to {result_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Word2Vec 적용 시 코드\n",
    "\n",
    "# 결과 파일 저장\n",
    "result_filename = f\"test_media_predictions(Word2Vec)_{keyword}_accuracy_{test_accuracy:.2f}.csv\"\n",
    "result.to_csv(result_filename, index=False)\n",
    "print(f\"Result saved to {result_filename}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
